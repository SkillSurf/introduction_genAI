{"cells":[{"cell_type":"markdown","metadata":{"id":"jGHnHFav4lGi"},"source":["# Projection, Layernorm, Dropout\n","\n","<table align=\"left\">\n","  <td>\n","    <a href=\"https://colab.research.google.com/github/SkillSurf/introduction_genAI/blob/main/notebooks/S1/Session_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","  </td>\n","</table>\n","\n","<br />\n","<br />"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5884,"status":"ok","timestamp":1717834003805,"user":{"displayName":"Abarajithan Gnaneswaran","userId":"15065416675145289008"},"user_tz":420},"id":"TzvEm56B4cd1"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"markdown","metadata":{"id":"rhgC0PRC5a8Q"},"source":["### Final Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-hPaOJH5aFU"},"outputs":[],"source":["batch_size = 64 # B: how many independent sequences will we process in parallel?\n","block_size = 256  # T: what is the maximum context length for predictions?\n","max_iters = 5000\n","eval_interval = 500\n","learning_rate = 1e-4\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_head = 6\n","n_embed = 64*n_head\n","n_layer = 6\n","dropout = 0.2\n","torch.manual_seed(1337)"]},{"cell_type":"markdown","metadata":{"id":"I8R79OYz4qFw"},"source":["### Hyperparameters"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":250,"status":"ok","timestamp":1717834006199,"user":{"displayName":"Abarajithan Gnaneswaran","userId":"15065416675145289008"},"user_tz":420},"id":"KVYFkS4Z4ojJ","outputId":"b405e545-f21c-4e8f-9c00-b900fd902949"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7a63841ce270>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["batch_size = 32 # B: how many independent sequences will we process in parallel?\n","block_size = 8  # T: what is the maximum context length for predictions?\n","max_iters = 5000\n","eval_interval = 500\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embed = 32\n","n_head = 4\n","n_layer = 4\n","dropout = 0.2\n","torch.manual_seed(1337)"]},{"cell_type":"markdown","metadata":{"id":"yLFQvyFf4taN"},"source":["### Data"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1717834024097,"user":{"displayName":"Abarajithan Gnaneswaran","userId":"15065416675145289008"},"user_tz":420},"id":"NwgGJ0-K4rx2","outputId":"918978b3-85ae-4305-eb28-9e6cbf8abba2"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-06-08 08:07:03--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n","\n","2024-06-08 08:07:03 (123 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":516,"status":"ok","timestamp":1717834035573,"user":{"displayName":"Abarajithan Gnaneswaran","userId":"15065416675145289008"},"user_tz":420},"id":"Hbw16RqT4wFg","outputId":"c8a2f10b-2198-4b33-f5c3-e34d593c6cbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["vocab_size: 65\n","vocabulary: \n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"]}],"source":["with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","chars_str = ''.join(chars)\n","print(f'vocab_size: {vocab_size}')\n","print(f'vocabulary: {chars_str}')\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n"]},{"cell_type":"markdown","metadata":{"id":"djRweqFN43In"},"source":["### Head, MHSA"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":224,"status":"ok","timestamp":1717834059206,"user":{"displayName":"Abarajithan Gnaneswaran","userId":"15065416675145289008"},"user_tz":420},"id":"84SgLLFc4xt6"},"outputs":[],"source":["class Head(nn.Module):\n","    \"\"\" One head of self attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key   = nn.Linear(n_embed, head_size, bias=False)\n","        self.query = nn.Linear(n_embed, head_size, bias=False)\n","        self.value = nn.Linear(n_embed, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, T, C  = x.shape\n","        '''\n","        B - batch               # of independant vectors processed\n","        T - time/block/context  # of tokens in a context\n","        C - vocab               # of possible tokens\n","        '''\n","\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","\n","        # compute attention scores / affinities\n","        wei = q @ k.transpose(-2,-1)                                 # (B,T,C) @ (B,C,T) -> (B,T,T)\n","        wei /= C**0.5                                                # (B,T,T) scaling, bring variance to 1, to prevent softmax clipping\n","        wei  = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))   # (B,T,T) Replace upper triangular of wei with -inf\n","        wei  = F.softmax(wei, dim=-1)                                # (B,T,T) -inf -> 0, rest normalized to 1\n","        wei  = self.dropout(wei)\n","\n","        v = self.value(x)  # (B,T,C)\n","        out = wei @ v      # (B,T,T) @ (B,T,C) = (B,T,C)\n","\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, n_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([      # 4 heads of 8-dimensional self-attention, for n_embed=32, like a group convolution\n","            Head(head_size) for _ in range(n_heads)\n","            ])\n","        self.proj = nn.Linear(n_embed, n_embed)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = torch.cat([h(x) for h in self.heads], dim=-1)\n","        x = self.proj(x)\n","        x = self.dropout(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"Om6pUr4N463t"},"source":["### Transformer Block"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1717834076894,"user":{"displayName":"Abarajithan Gnaneswaran","userId":"15065416675145289008"},"user_tz":420},"id":"2m3aC0tt44sv"},"outputs":[],"source":["class Block(nn.Module):\n","    ''' Transformer block: communication followed by computation '''\n","\n","    def __init__(self, n_embed, n_head): # n_embed: embedding dimension, n_head: number of heads\n","        super().__init__()\n","        head_size = n_embed // n_head\n","        self.ln1 = nn.LayerNorm(n_embed)   # Layernorm along channels (batch & time are batch dims): y = beta + gamma * [x-E(x)]/sqrt(V(x) + ep)\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ln2 = nn.LayerNorm(n_embed)\n","        self.ffwd = nn.Sequential(         # Feedforward network, so the tokens can \"think about\" what they found in attention.\n","            nn.Linear(n_embed, n_embed*4),\n","            nn.ReLU(),\n","            nn.Linear(n_embed*4, n_embed),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        # Residual connections around MSA & FF, to help training\n","        # Note: input without layernorm is added to output\n","        x = x + self.sa(self.ln1(x))                                     # (B,T,C), Multi head self attention\n","        x = x + self.ffwd(self.ln2(x))                                   # (B,T,C), Per token level. B,T act as batch dimensions\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"rEZ6CZrm5ADW"},"source":["### Model"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":240,"status":"ok","timestamp":1717834094092,"user":{"displayName":"Abarajithan Gnaneswaran","userId":"15065416675145289008"},"user_tz":420},"id":"8B84EGMO49AA"},"outputs":[],"source":["class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # for every possible token, weights for next token\n","        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n","\n","        self.blocks  = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n","        self.ln_final = nn.LayerNorm(n_embed)\n","        self.lm_head = nn.Linear(n_embed, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        tok_emb = self.token_embedding_table(idx)                                        # (B,T,C=n_embed)\n","        pos_emb = self.position_embedding_table(torch.arange(block_size, device=device)) # (T,C): [0,1,2..T-1]\n","\n","        x = tok_emb + pos_emb     # (B,T,C)\n","        x = self.blocks(x)\n","        x = self.ln_final(x)      # Layernorm applied before last\n","        logits = self.lm_head(x)  # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        for _ in range(max_new_tokens):                        # idx is (B, T) array of indices in the current context\n","            idx_cond = idx[:, -block_size:]                    # crop the last block_size tokens for input\n","            logits, loss = self(idx_cond)                      # get the predictions\n","            logits = logits[:, -1, :]                          # (B,T,C) -> (B, C)\n","            probs = F.softmax(logits, dim=-1)                  # (B, C)\n","            idx_next = torch.multinomial(probs, num_samples=1) # sample from the distribution acc to prob (B, 1)\n","            idx = torch.cat((idx, idx_next), dim=1)            # New idx is concat (B, T+1)\n","        return idx\n","\n","model = BigramLanguageModel()\n","m = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"GqlT4Bwa5Dfo"},"source":["### Training"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175313,"status":"ok","timestamp":1717834288448,"user":{"displayName":"Abarajithan Gnaneswaran","userId":"15065416675145289008"},"user_tz":420},"id":"9pDDJiTV5BPa","outputId":"bbb95283-5b18-409e-e051-6791904a0c37"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 4.2318, val loss 4.2261\n","step 500: train loss 2.4590, val loss 2.4603\n","step 1000: train loss 2.3157, val loss 2.3389\n","step 1500: train loss 2.2520, val loss 2.2673\n","step 2000: train loss 2.2006, val loss 2.2348\n","step 2500: train loss 2.1819, val loss 2.1976\n","step 3000: train loss 2.1352, val loss 2.1731\n","step 3500: train loss 2.1150, val loss 2.1408\n","step 4000: train loss 2.1086, val loss 2.1305\n","step 4500: train loss 2.0726, val loss 2.1317\n"]}],"source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","    if iter % eval_interval == 0:   # every once in a while evaluate the loss on train and val sets\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    xb, yb = get_batch('train')     # sample a batch of data\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"tLhWVvJR5Hmv"},"source":["### Inference"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16629,"status":"ok","timestamp":1717834351507,"user":{"displayName":"Abarajithan Gnaneswaran","userId":"15065416675145289008"},"user_tz":420},"id":"oArZB_RT5FHn","outputId":"03edc92c-ec0e-4dae-edc2-f428e4aa5a68"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\n","Yout thou chafpoucht gorts.\n","\n","Comedemen,\n","Thamon not he in I your.\n","Buore hendsontely he prieliveal of wer my the morfome mesing, oncaponk'd the in sulders victh and at ppacth asut in thee latily\n","Thoulv no salred gort there norm o dege usays a ard pust good in the, prisesetay, to door, heste nimess of herme seest I mardetings hort,\n","I snoill.\n","Thee thought yas the my maviow.\n","\n","Sthy gilliebetlor, Ro Romquiefiind,\n","Soned am,\n","In prown bordy artuck here,\n","Why a ulio'th rie reathy or your venoire\n","And in if do dales,\n","Wherestref; tlild, tond mut of wericn?\n","Fronm so.\n","\n","HARD:\n","Thiptriop toi;\n","Tut fear poot shimes a vice,\n","And Clohmotten jove cavue huspamin night.\n","\n","KINGARD: howelf att rwome, is stpan it lordere; I srugiom. Clast I a ner of upeligion,.\n","Wer am preceintat you quen they?\n","\n","StCALLIURS:\n","To huntmer thour with a waed\n","Icingalliue bintbay\n","dereters:\n","O is dithat so home;\n","Aut.\n","\n","AK:Cele camaner, am tisof daed? I'll tempann uchat alf shour coment mazinty to nal, and ond. And to lurst stromed ith gring Leltelr?\n","\n","Flove I my's of word,\n","Id tattronter; of that, deeenb goute,\n","Anvl shought me I botederit:\n","Rerese, Om; ix and a,\n","Sint\n","Belison! tinke like ath ther I cown come'd cangeer,\n","And in a Jond\n","not is ill and of me, eve piome\n","'thy, hime lecle palllaces, would seeied revy, hould kay comughis his, my bree this me selvosh ey.\n","\n","ORDu\n","Swill for veore hold labusere yous, bapitet thello, breeat have, uppers hown are not youll siedd creack\n","Luck say, lonser here not this say notten a\n","KISito wither'd shererve\n","that and doth duloaaintmy.\n","MINES:\n","\n","KIZING thend ouvers ourights ame will yeet at plear mebillut:\n","Shoun annd, to ceivel at you, prood'd me roved'd ay'le melom, comerther, Muld meie in s faplead binge drue, bake'll herre.\n","-I I all we anasst,\n","Or with now'd dacace; seed, to co the larsigios' as ing\n","Noner abowh pon thestill'd fake?\n","\n","FLeels your all ther you of quove ve huch'chat ald, Prouese comake? I nurd in entigssy selle of,\n","Thergure ste to with. hom will, be I poinesiof? Gad, thrue.\n","\n","At the miyour \n"]}],"source":["context = torch.zeros((1, block_size), dtype=torch.long, device=device)  # start with '\\n\\n\\n\\n' as seed\n","out_ints = m.generate(context, max_new_tokens=2000)[0].tolist() # output list of ints\n","print(decode(out_ints))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fNpRB1u5IuS"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMZrG7NQ4L8Sq0QalRWFy37","collapsed_sections":["rhgC0PRC5a8Q"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
